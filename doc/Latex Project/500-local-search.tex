\chapter{Proposed Approach: Local Search}
\label{sec:LocalSearch}
\thispagestyle{plain}

The proposed solution consists about using a local search meta-heuristic(s) to improve the solution given by the \gls{gc} heuristic. This approach uses \gls{sa}, based on M\"{u}ller's approach \cite{Mueller2009}, followed by the use of \gls{hc}. The use of \gls{hc} is justified by the fact that \gls{sa} does not guarantee a good control of the execution time, and so the parameters are given to make it run almost all the available time. The rest of the optimization is executed on \gls{hc}, whose execution time is perfectly controllable.

\section{Simulated Annealing}
\label{sec:SimulatedAnnealing}

\gls{sa} is a single-solution meta-heuristic (section \ref{metaheuristics}). This meta-heuristic optimizes a solution by generating neighbor solutions which might be accepted given an acceptance criterion. A neighbor solution is obtained by applying a movement operator (also known as neighbor operator) to the current solution, creating a new solution which is a neighbor of the current solution. A neighbor operator, in this context, could be the movement of an examination to another time slot. Being a single-solution meta-heuristic, it only generates one neighbor at the time. The neighbor operators and acceptance criterion are the most important part of this algorithm. Little changes on one of these, may get the algorithm to behave in very different ways and end up with much different solutions.\\
\\
The acceptance criterion will, considering the current solution and a neighbor solution, give the percentage of acceptance of the neighbor solution. Most of the approaches using this heuristic accept a new neighbor solution if this is \textit{better} than the current solution. Otherwise, there's a chance that the neighbor solution is still accepted, depending on certain parameters. These parameters are the \textit{Temperature} (normally given as maximum and minimum temperature) and the \textit{Cooling Schedule}. By definition, the higher the temperature, the higher is the chance to accept a worse solution over the current solution. The cooling schedule, as the name suggests, is a function that lowers the temperature at a given rate. The SA algorithm finishes when the current temperature is lower or equal to the minimum temperature. The temperature should start high enough in order to be able to escape from local optima, by accepting worse solutions found during the trajectory.

\subsection{Implementation}

The \gls{sa} was implemented in a way that it is independent from the type of the cooling schedule and neighbor generator. The \gls{sa} base class is abstract and implements everything except for the neighbor generator, which is an abstract method that must be implemented in order to decide how the neighbor generator behaves. It also does not implement the evaluation function (the one that computes the fitness value of a solution or neighbor). The \verb+SimulatedAnnealing+ and \verb+SimulatedAnnealingTimetable+'s methods and variables can be seen in Figure \ref{fig:SimulatedAnnealing}.\\
\\
\begin{figure}[p!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 
\umlabstract[x = 0]{SimulatedAnnealing}
{
	\umlvirt{\#evaluation\_function : IEvaluationFunction}\\
	-cooling\_schedule : ICoolingSchedule
}
{
	+Exec(solution : ISolution, TMax : double, \\ TMin : double, loops : int, type : int, minimize : bool)\\
	+Exec2(solution : ISolution, TMax : double, \\ TMin : double, loops : int, rate : double, type : int, minimize : bool)\\
	+ExecLinearTimer(solution : ISolution, TMax : double,\\ TMin : double, milliseconds : long, type : int, long : minimize)\\
	\umlvirt{\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor}\\
	\umlvirt{\#InitVals(type : int)}
}
\umlclass[x = 0, y = -8]{SimulatedAnnealingTimetable}
{
	\#evaluation\_function : IEvaluationFunction\\
	-neighbor\_selection\_timetable : NeighborSelectionTimetable\\
	+\umlstatic{type\_random : int}\\
	+\umlstatic{type\_guided1 : int}\\
	+\umlstatic{type\_guided2 : int}\\
	-room\_change : int\\
	-period\_change : int\\
	-period\_room\_change : int\\
	-room\_swap : int\\
	-period\_swap : int\\
	-period\_room\_swap : int\\
	+generated\_neighbors : int\\
	-random : Random\\
	-total\_neighbor\_operators : int
}
{
	\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor\\
	\#GenerateNeighbor(solution : Solution, type : int) : INeighbor\\
	-GenerateRandomNeighbor(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor1(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor2(solution : Solution) : INeighbor\\
	\#InitVals(type : int)\\
	+EstimateTotalNumberOfNeighbors(average\_reps : int, \\ total\_time : int, solution : Solution)
}
\end{umlpackage}


\umlimpl[anchors=90 and -90]{SimulatedAnnealingTimetable}{SimulatedAnnealing} 
\end{tikzpicture}

\caption{Simulated Annealing classes} 
\label{fig:SimulatedAnnealing}
\end{figure}
The \verb+SimulatedAnnealing+ abstract class has the methods \verb+Exec+, \verb+Exec2+, and \verb+ExecLinearTimer+, which are all similar, but were created to test different approaches. All these methods share the same code, in exception to the cooling schedule (the way the temperature is updated) and acceptance criterion. The pseudo code of these can be seen in Algorithm \ref{alg:SimulatedAnnealing}.\\
\begin{algorithm}[t!]
\textbf{Input:} 
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\item $s$ \Comment{Initial solution}
	\item $TMax$ \Comment{Maximum temperature}
	\item $TMin$ \Comment{Minimum temperature}
	\item $loops$ \Comment{Number of loops per temperature}
\end{itemize}
\begin{algorithmic}
\State $T = Tmax$ ; \Comment{Starting temperature}
\State $Ac = AcInit()$ ; \Comment{Acceptance criterion initializer}
\Repeat
	\Repeat	
		\State Generate a random neighbor $s'$;
		\State $\delta E$ = $f(s') - f(s)$ ;
		\State \textbf{If} $E \leq 0$ \textbf{Then} $s = s'$ \Comment{Accept the neighbor solution}
		\State \textbf{Else} Accept $s'$ with a probability computed using the $Ac$;
	\Until Number of iterations reach $loops$
	\State $T = g(T )$ \Comment{Temperature update}
\Until $T < TMin$
\State \textbf{Output:} $s$ \Comment{return the current (best) solution}
\end{algorithmic}
\caption{Simulated Annealing method.}
\label{alg:SimulatedAnnealing}
\end{algorithm}\\
The \verb+ExecLinearTimer+ has a linear cooling schedule, which is proportional to the running time, and uses the acceptance criterion:\\
\[P(\delta E, T) = e^{\frac{-\delta E}{T}} \]
T $\rightarrow$ Current temperature\\
$\delta$E $\rightarrow$ Fitness difference between the new neighbor and current solution\\
\\
The \verb+Exec+ method shares the same acceptance criterion but uses a geometric cooling schedule:\\
\[T (i+1) = T(i).r \]
r $\rightarrow$ Temperature decreasing rate (0 < r < 1).\\
\\
The rate must belong in the interval ]0,1[. In the geometric cooling schedule, the closer the rate is to unity, the longer the algorithm takes to finish and wider is the area of solutions to be analyzed in the solution space.\\
\\
The \verb+Exec2+ method is the one used in this project. It uses an exponential (decreasing) cooling schedule \cite{CarvalhoLisbonNovember2004}:\\
\[T = T_{max}e^{-R.t} \]
t $\rightarrow$ Current span, counter initiated at 0\\
T\textsubscript{max} $\rightarrow$ Maximum/initial temperature\\
R $\rightarrow$ Decreasing rate.\\
\\
This method also uses a different acceptance criterion:\\
\[P(\delta E, T, SF) = e^{\frac{-\delta E}{T.f(s)}} \]
f(s) $\rightarrow$ Solution fitness\\
\\
The behavior of this heuristic depends on the given parameters (maximum and minimum temperature, rate, number of loops per temperature, type of the cooling schedule), and the difficulty of the set is also important, considering the execution time limit. Knowing this, it's important to choose the parameters wisely to obtain the best possible results. But, each set has its own characteristics and using parameters for one set does not guarantee that the results will be equivalently good for the others. One example is finding the best parameters for the first set, and make it try run for about 200 milliseconds. This does not mean the others will run for about the same amount of time, degrading the effectiveness of this heuristic. Thus, a variable rate was implemented.\\
\\
The variable rate was implemented to make this heuristic run closer to the given time limit. Considering it is not certain that the algorithm will run within the given 221000 milliseconds, as being a stochastic algorithm, a time limit was added to this meta-heuristic as well, ending this heuristic automatically if the time limit is reached. If this happens, it degrades the meta-heuristic's performance because it ends before it's suppose to. To avoid this, an offset was created. The offset is a percentage which is removed from a part of the simulation to, instead of pointing the heuristic to run for 221000 milliseconds, it points to 185640 milliseconds, to have a margin of safety. The offset used is 16\%.\\
\\
The algorithm starts by simulating the execution of this heuristic, by running all the neighbor operators \textit{n} times and using the elapsed time and the given time limit, we estimate the number of neighbors that would be generated if this heuristic was to be run within the time limit. To calculate the said number of total neighbors, we use this math expression: $CompNeighbs = TotalTime / CompTime * AverageReps * TotalOperats$. After that, we compute the exact number of the neighbors ($TotalNeighbs$) that will be generated for the given parameters, using the default rate. This is achieved by simulating this heuristic using those parameters, cooling the current temperature until it reaches the minimum and returning the number supposed generated neighbors. If the $TotalNeighbs$ is way higher than the $CompNeighbs$, a lower rate will be used to make another simulation, until the $TotalNeighbs$ for the given rate reaches a value that is close to the $TotalNeighbs$. The pseudo code of this method can be seen in Algorithm \ref{alg:RateComputing}.\\
\\
\begin{algorithm}[t!]
\textbf{Input:} 
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\item $s$ \Comment{Solution}
	\item $TMax$ \Comment{Maximum temperature}
	\item $TMin$ \Comment{Minimum temperature}
	\item $reps$ \Comment{Number of loops per temperature}
	\item $exec\_time$ \Comment{Execution time limit}
\end{itemize}
\begin{algorithmic}
\State $sa = SAInit()$ ;
\State $n = 50$ ; \Comment{Number of times each operator runs}
\State $est\_neighbors = sa.EstimateNumberNeighbors(n, exec\_time, s)$ ;
\State $rate = 1.50^{-04}$ ;\Comment{Initial default rate}
\State $power = -4$ ;
\State $rate\_to\_sub = 10^{power}$ ;\Comment{Rate decrementing}
\State $total\_neighbors = sa.GetNumberNeighbors(TMax, rate, reps, TMin)$ ;
\Repeat	
	\State \textbf{If} $rate - rate\_to\_sub \leq 0$ \textbf{Then} $power = power - 1$ ; $rate\_to\_sub = 10^{power}$ ;
	\State $rate = rate - rate\_to\_sub$ ;
	\State $total\_neighbors = sa.GetNumberNeighbors(TMax, rate, reps, TMin)$ ;
\Until $total\_neighbors < est\_neighbors$
\State $rate = rate + rate\_to\_sub$ ; \Comment{To guarantee that total\_neighbors < est\_neighbors}
\State \textbf{Output:} $rate$ \Comment{Return computed rate}
\end{algorithmic}
\caption{Rate computing.}
\label{alg:RateComputing}
\end{algorithm}Some testings were made in order to check this heuristic's behavior, using the following parameters: \textit{TMax} = 0.1, \textit{TMin} = $1e-06$, \textit{loops} = 5, \textit{exec\_time} = 50000, and the computed\_rate = 0.00016. As can be seen in Figure \ref{fig:SimulatedAnnealingPlot}, it starts by accepting all worse and better neighbor solutions. In the end, the temperature is so low that it becomes harder to accept worse solutions, ending up acting similar to the \gls{hc} procedure.\\
\\
\begin{figure}[!t]
\centering

\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={Neighbor index},
    ylabel={Solution score},
    mark size=0.2pt
]

\addplot plot file [
    color=blue,
    mark=*,
    mark options={solid},
   	smooth
    ] {sa_plot_data.dat};

\end{axis}
\end{tikzpicture}

\caption{Simulated Annealing results: score value as a function of the temperature} 
\label{fig:SimulatedAnnealingPlot}
\end{figure}
The tests performed on all the sets are shown and explained in Chapter \ref{chap:ExpResults}.

\section{Hill Climbing}
\label{sec:HillClimbing}

\gls{hc} is a meta-heuristic different to the \gls{sa} family of meta-heuristics, in the sense that only accepts improving solutions. So, as long as it reaches a local optimum, it can't get out of that point because every neighbor solutions are worse. In this way, \gls{hc} only has one parameter that is the number of iterations or time limit which controls the algorithms' execution time.\\
\\
In the evaluation undertaken, the best results were obtained with the \verb+Exec2+ version of \gls{sa}. \gls{sa} was parametrized in order to finish execution within the specified time limit imposed by the \gls{itc2007} rules. \gls{hc} is executed after \gls{sa} until the time limit is reached.

\subsection{Implementation}
The implementation of this heuristic is very similar to the \gls{sa}, also containing the classes \verb+HillClimbing+ and \verb+HillClimbingTimetable+, which are simplified versions of the \gls{sa} classes. The \verb+HillClimbing+ and \verb+HillClimbingTimetable+'s methods and variables can be seen in Figure \ref{fig:HillClimbing}.\\
\\
\begin{figure}[t!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 
\umlabstract[x = 0]{HillClimbing}
{
	\umlvirt{\#evaluation\_function : IEvaluationFunction}\\
}
{
	+Exec(solution : ISolution, milliseconds : long, type : int, \\ minimize : bool)\\
	\umlvirt{\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor}
}
\umlclass[x = 0, y = -5]{HillClimbingTimetable}
{
	\#evaluation\_function : IEvaluationFunction\\
	-neighbor\_selection\_timetable : NeighborSelectionTimetable\\
	+\umlstatic{type\_random : int}\\
	-random : Random\\
	+generated\_neighbors : int\\
	-total\_neighbor\_operators : int\\
}
{
	\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor\\
	\#GenerateNeighbor(solution : Solution, type : int) : INeighbor\\
	-GenerateRandomNeighbor(solution : Solution) : INeighbor\\
}
\end{umlpackage}


\umlimpl[anchors=90 and -90]{HillClimbingTimetable}{HillClimbing} 
\end{tikzpicture}

\caption{Hill Climbing classes} 
\label{fig:HillClimbing}
\end{figure}

\section{Neighborhood Operators}
\label{sec:NeighborhoodOperators}

Neighborhood operators are operations applied to a solution, in order to create other valid solutions (neighbor solutions), but not necessarily feasible. In this context, the core of all operations are the relocation of the examinations.\\
\\
The implementation of neighborhood selection went through three different approaches. Firstly, the random selection was implemented. This approach always chooses a random operator to generate a new neighbor. Thereafter two guided approaches were implemented. The first one raised the probability of selecting one operator if this one generated a better neighbor solution. The probability of that operator is reduced in an equal amount if the operator generated a worse solution. The other guided approach simply lowers the probability of an operator in case of success, and raises it in case of unsuccess. \\
\\
The second guided approach presented worse results compared to the first one, as expected. Oddly, the random approach almost always showed better results compared to the first guided approach, even after suffering major changes in order to try to get it to generate better results. These changes include raising the probabilities even more or less when successfully or unsuccessfully created a better solution, and instead of lowering the probability for an operator when created a worse solution, it is returned to its default value.\\
\\
The neighborhood operators, in this context, are all based on moving examinations to another place (period or room). This implementation uses six different neighborhood operators:
\\
\begin{itemize}
	\item \textit{Room Change} - A random examination is selected. After that, a random room is randomly selected. If the assignment of the random examination to the random room, while maintaining the period, does not violate any hard constraints, that neighbor is returned. If not, the adjacent rooms are tested until one of them creates a feasible solution. If it reaches the limit of rooms and no feasible solution is found, no neighbor is returned;\\
	
	\item \textit{Period Change} - A random examination is selected. After that, a random period is randomly selected. If the assignment of the random examination to the random period, while maintaining the room, does not clash with any hard constraints, that neighbor is returned. If not, the adjacent periods are sequentially tested until one of them creates a feasible solution. If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Change} - A random examination is selected. After that, a random room and period are randomly selected. If the assignment of the random examination to the random room and period does not clash with any hard constraints, that neighbor is returned. If not, the next rooms are tested for each of the next periods, until one of them creates a feasible solution. If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Room Swap} - A random examination is selected. After that, a random room is selected. If the selected examination can be placed in that room, while maintaining the period, then a \textit{Room Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random room, keeping the same period, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next rooms are tested until a feasible solution is found (always testing first if a \textit{Room Change} can be returned instead). If it reaches the limit of rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period Swap} -  A random examination is selected. After that, a random period is selected. If the selected examination can be placed in that period, while maintaining the room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period, keeping the same room, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods are tested until a feasible solution is found (always testing first if a \textit{Period Change} can be returned instead). If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Swap} - A random examination is selected. After that, a random period and room are selected. If the selected examination can be placed in that period and room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period and room does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods and rooms are tested until a feasible solution is found (always testing first if a \textit{Period \& Room Change} can be returned instead). If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;
\end{itemize}

\subsection{Implementation}

The original concept of neighbor solution is to have another solution apart from the current one, which is the result of applying the neighborhood operator to the current solution. In order to have an efficient implementation, the neighbor only keeps the changes introduced in the solution, and not the changed solution. With this design, there's no need to replicate the original solution and apply the neighborhood operator to it in order to obtain the neighbor solution. The process of replacing the original solution with the neighbor, consists in applying to the current solution the movement registered in the neighbor.\\
\\
Every neighbor object must implement the interface \verb+INeighbor+, which presents the methods \verb+Accept+, \verb+Reverse+ and a real value that represents the fitness of the neighbor (the fitness of the new solution if this neighbor is to be accepted). The \verb+Accept+ method  moves the current solution to the neighbor solution. The \verb+Reverse+ method undoes the operation, getting then back the old solution. The different neighbors and their methods are illustrated in Figure \ref{fig:Neighbors}.\\
\begin{figure}[p!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 

\umlclass[x = 0, y = 0]{SimulatedAnnealingTimetable}{}{}
\umlclass[x = 0, y = -4]{NeighborSelectionTimetable}
{
	-examinations : Examinations\\
	-rooms : Rooms\\
	-periods : Periods\\
	-feasibility\_tester : FeasibilityTester\\
	-evaluation\_function\_timetable : EvaluationFunctionTimetable\\
}
{
	+RoomSwap(solution : Solution) : INeighbor\\
	+PeriodSwap(solution : Solution) : INeighbor\\
	+PeriodRoomSwap(solution : Solution) : INeighbor\\
	+RoomChange(solution : Solution) : INeighbor\\
	+PeriodChange(solution : Solution) : INeighbor\\
	+PeriodRoomChange(solution : Solution) : INeighbor\\
}
\umlclass[x = 0, y = -9]{INeighbor}
{
	+fitness : int\\
}
{
	+Accept() : ISolution\\
	+Reverse() : ISolution
}
\umlclass[x = -3, y = -11]{PeriodChangeNeighbor}{}{}
\umlclass[x = -3, y = -13]{RoomChangeNeighbor}{}{}
\umlclass[x = -3, y = -15]{PeriodRoomChangeNeighbor}{}{}
\umlclass[x = 3, y = -11]{PeriodSwapNeighbor}{}{}
\umlclass[x = 3, y = -13]{RoomSwapNeighbor}{}{}
\umlclass[x = 3, y = -15]{PeriodRoomSwapNeighbor}{}{}


\end{umlpackage}



\umlimpl[geometry=-|, anchors=180 and -90]{PeriodChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{RoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{PeriodRoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{RoomSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodRoomSwapNeighbor}{INeighbor}
\umlinclude[anchors=-90 and 90, name=uses]{SimulatedAnnealingTimetable}{NeighborSelectionTimetable}
\umlinclude[anchors=-90 and 90, name=uses]{NeighborSelectionTimetable}{INeighbor} 
\end{tikzpicture}

\caption{Neighborhood selection and operators} 
\label{fig:Neighbors}
\end{figure}

\section{Fitness Computation}
\label{sec:FitnessComputation}

The fitness computation is an important topic, when taking performance into consideration. As mentioned in the Subsection \ref{subsec:ToolsLayer}, this value is computed using the \verb+EvaluationFunction+ tool, which in a first approach, implied for generated neighbors, the recalculation of their fitness value. Tests were made to study this approach since the fitness results were not as good as expected. The tests showed that this approach was having poor performance since most of the time was used to compute the fitness for each new neighbor generated solution.\\
\\
A new approach was implemented, which consists in computing the fitness incrementally. This means for each new generated neighbor, a new fitness value is computed based on the current solution's fitness. This approach takes advantage of the fact that the operations applied to the current solution are simple and so the neighbor solutions are, in most part, equal to the current solution. Thus, the fitness computation is performed considering only the small changes between the current solution and the new generated neighbor solution.\\
\\
Tests were made to compare both approaches and, using the same parameters, for the first set it was possible to achieve equivalent results, nineteen times faster. This means that if both approaches would use the same parameters, but the new approach using a lower rate to match the execution time of both approaches, the new approach would have generated nineteen times more neighbors.

\subsection{Implementation}

To compute the fitness of the neighbor solutions incrementally, a fitness function must be implemented depending on the type of neighbor. Considering all neighbors have a reference to the solution they derive from, and the methods \verb+Accept+ and \verb+Reject+, it's possible to access the current solution and the neighbor generated solution (Reminding that the \verb+Accept+ will change the current solution and so the reference presented in this neighbor, replacing with the neighbor solution). \\
\\
The differences between the fitness values of the current solution and the new neighbor's depend on the type of the neighbor created. For example, if a \verb+RoomSwapNeighbor+ is created, the changes will occur in the mixed durations constraints value of the periods and rooms of the swapped examinations. It is necessary to compute the impact (fitness values) of these conflicts in the current solution and subtract its value to the current fitness, proceeding with computing the same impact but now for the generated neighbor and sum its value to the current fitness, thus obtaining the new neighbor's fitness value. The impact computing mentioned above is a simple rule that is must be followed by all the soft constraints when computing the new neighbor's fitness incrementally.