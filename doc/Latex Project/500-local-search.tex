\chapter{Proposed Approach: Local Search}
\label{sec:LocalSearch}
\thispagestyle{plain}

The proposed solution consists about using a local search meta-heuristic(s) to improve the solution given by the \gls{gc} heuristic. This approach uses \gls{sa}, based on M\"{u}ller's approach \cite{Mueller2009}, followed by the use of \gls{hc}. The use of \gls{hc} is justified by the fact that \gls{sa} does not guarantee a good control of the execution time, and so the parameters are given to make it run almost all the available time. The rest of the optimization is executed on \gls{hc}, whose execution time is perfectly controllable.

\section{Simulated Annealing}
\label{sec:SimulatedAnnealing}

\gls{sa} is a single-solution meta-heuristic (section \ref{metaheuristics}). This meta-heuristic optimizes a solution by generating neighbor solutions which might be accepted given an acceptance criterion. A neighbor solution is obtained by applying a movement operator (also known as neighbor operator) to the current solution, creating a new solution which is a neighbor of the current solution. A neighbor operator, in this context, could be the movement of an examination to another time slot. Being a single-solution meta-heuristic, it only generates one neighbor at the time. The neighbor operators and acceptance criterion are the most important part of this algorithm. Little changes on one of these, may get the algorithm to behave in very different ways and end up with much different solutions.\\
\\
The acceptance criterion will, considering the current solution and a neighbor solution, give the percentage of acceptance of the neighbor solution. Most of the approaches using this meta-heuristic accept a new neighbor solution if this is \textit{better} than the current solution. Otherwise, there's a chance that the neighbor solution is still accepted, depending on certain parameters. These parameters are the \textit{Temperature} (normally given as maximum and minimum temperature) and the \textit{Cooling Schedule}. By definition, the higher the temperature, the higher is the chance to accept a worse solution over the current solution. The cooling schedule, as the name suggests, is a function that lowers the temperature at a given rate. The SA algorithm finishes when the current temperature is lower or equal to the minimum temperature. The temperature should start high enough in order to be able to escape from local optima, by accepting worse solutions found during the trajectory.

\subsection{Implementation}

The \gls{sa} was implemented in a way that it is independent from the type of the cooling schedule and neighbor generator. The \gls{sa} base class is abstract and implements everything except for the neighbor generator, which is an abstract method that must be implemented in order to decide how the neighbor generator behaves. It also does not implement the evaluation function (the one that computes the fitness value of a solution or neighbor). The \verb+SimulatedAnnealing+ and \verb+SimulatedAnnealingTimetable+'s methods and variables can be seen in Figure \ref{fig:SimulatedAnnealing}.\\
\\
\begin{figure}[p!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 
\umlabstract[x = 0]{SimulatedAnnealing}
{
	\umlvirt{\#evaluation\_function : IEvaluationFunction}\\
	-cooling\_schedule : ICoolingSchedule
}
{
	+Exec(solution : ISolution, TMax : double, \\ TMin : double, loops : int, type : int, minimize : bool)\\
	+Exec2(solution : ISolution, TMax : double, \\ TMin : double, loops : int, rate : double, type : int, minimize : bool)\\
	+OnlyBetter(solution : ISolution, \\ milliseconds : long, type : int, minimize : bool)\\
	+ExecLinearTimer(solution : ISolution, TMax : double,\\ TMin : double, milliseconds : long, type : int, long : minimize)\\
	\umlvirt{\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor}\\
	\umlvirt{\#InitVals(type : int)}
}
\umlclass[x = 0, y = -8]{SimulatedAnnealingTimetable}
{
	\#evaluation\_function : IEvaluationFunction\\
	-neighbor\_selection\_timetable : NeighborSelectionTimetable\\
	+\umlstatic{type\_random : int}\\
	+\umlstatic{type\_guided1 : int}\\
	+\umlstatic{type\_guided2 : int}\\
	-room\_change : int\\
	-period\_change : int\\
	-period\_room\_change : int\\
	-room\_swap : int\\
	-period\_swap : int\\
	-period\_room\_swap : int\\
}
{
	\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor\\
	\#GenerateNeighbor(solution : Solution, type : int) : INeighbor\\
	-GenerateRandomNeighbor(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor1(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor2(solution : Solution) : INeighbor\\
	\#InitVals(type : int)
}
\end{umlpackage}


\umlimpl[anchors=90 and -90]{SimulatedAnnealingTimetable}{SimulatedAnnealing} 
\end{tikzpicture}

\caption{Simulated Annealing classes} 
\label{fig:SimulatedAnnealing}
\end{figure}
The \verb+SimulatedAnnealing+ abstract class has the methods \verb+Exec+, \verb+Exec2+, \verb+OnlyBetter+, and \verb+ExecLinearTimer+, which are all similar, but were created to test different approaches. All these methods share the same code, in exception to the cooling schedule (the way the temperature is updated) and acceptance criterion. The pseudo code of these can be seen in Algorithm \ref{alg:SimulatedAnnealing}.\\
\begin{algorithm}[b!]
\textbf{Input:} 
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\item $s$ \Comment{Initial solution}
	\item $TMax$ \Comment{Maximum temperature}
	\item $TMin$ \Comment{Minimum temperature}
	\item $loops$ \Comment{Number of loops per temperature}
\end{itemize}
\begin{algorithmic}
\State $T = Tmax$ ; \Comment{Starting temperature}
\State $Ac = AcInit()$ ; \Comment{Acceptance criterion initializer}
\Repeat
	\Repeat	
		\State Generate a random neighbor $s'$;
		\State $\delta E$ = $f(s') - f(s)$ ;
		\State \textbf{If} $E \leq 0$ \textbf{Then} $s = s'$ \Comment{Accept the neighbor solution}
		\State \textbf{Else} Accept $s'$ with a probability computed using the $Ac$;
	\Until Number of iterations reach $loops$
	\State $T = g(T )$ \Comment{Temperature update}
\Until $T < TMin$
\State \textbf{Output:} $s$ \Comment{return the current (best) solution}
\end{algorithmic}
\caption{Simulated Annealing method.}
\label{alg:SimulatedAnnealing}
\end{algorithm}\\
The \verb+ExecLinearTimer+ has a linear cooling schedule, which is proportional to the running time, and uses the acceptance criterion:\\
\[P(\delta E, T) = e^{\frac{-\delta E}{T}} \]
T $\rightarrow$ Current temperature\\
$\delta$E $\rightarrow$ Fitness difference between the new neighbor and current solution\\
\\
The \verb+Exec+ method shares the same acceptance criterion but uses a geometric cooling schedule:\\
\[T (i+1) = T(i).r \]
r $\rightarrow$ Temperature decreasing rate (0 < r < 1).\\
\\
The rate must belong in the interval ]0,1[. In the geometric cooling schedule, the closer the rate is to unity, the longer the algorithm takes to finish and wider is the area of solutions to be analyzed in the solution space.\\
\\
The \verb+Exec2+ method is the one used in this project. It uses an exponential (decreasing) cooling schedule \cite{CarvalhoLisbonNovember2004}:\\
\[T = T_{max}e^{-R.t} \]
t $\rightarrow$ Current span, counter initiated at 0\\
T\textsubscript{max} $\rightarrow$ Maximum/initial temperature\\
R $\rightarrow$ Decreasing rate.\\
\\
This method also uses a different acceptance criterion:\\
\[P(\delta E, T, SF) = e^{\frac{-\delta E}{T.f(s)}} \]
f(s) $\rightarrow$ Solution fitness\\
\\
Some testings on the parameters were performed, in order to check what parameters produce the best results. Due to the time taken, all the testings were made for the first set only, and so the same parameters will be used on all the sets so we can compare to results to other approaches, beginning with the five \gls{itc2007} finalists. Figure \ref{fig:SimulatedAnnealingPlot} illustrates an example of a plot of the \gls{sa} running with the following parameters: \textit{TMax} = 45, \textit{TMin} = $e^{-18}$, \textit{loops} = 5 and \textit{rate} = 0.01.\\
\\
\begin{figure}[!b]
\centering

\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={-log10(Temperature)},
    ylabel={Solution score},
]

\addplot plot file [
    color=blue,
    mark=o,
    ] {sa_plot_data.dat};

\end{axis}
\end{tikzpicture}

\caption{Simulated Annealing results: score value as a function of the temperature} 
\label{fig:SimulatedAnnealingPlot}
\end{figure}As can be seen in Figure \ref{fig:SimulatedAnnealingPlot}, it starts by accepting almost all better neighbor solutions, because in the beginning they are rather easy to find and are all accepted. In the middle, the score of the accepted neighbors are stable, because it accepts almost all worst solutions and the ratio of seeking better or worse solutions is very similar. In the end, the temperature is so low that it becomes harder to accept worse solutions, ending up acting similar to the \gls{hc} procedure.\\
\\
The tests performed on all the sets are shown and explained in Chapter \ref{chap:ExpResults}.

\section{Hill Climbing}
\label{sec:HillClimbing}

\gls{hc} is a meta-heuristic different to the \gls{sa} family of meta-heuristics, in the sense that only accepts improving solutions. So, as long as it reaches a local optimum, it can't get out of that point because every neighbor solutions are worse. In this way, \gls{hc} only has one parameter that is the number of iterations or time limit which controls the algorithms' execution time.\\
\\
In the evaluation undertaken, the best results were obtained with the \verb+Exec2+ version of \gls{sa}. \gls{sa} was parametrized in order to finish execution within the specified time limit imposed by the \gls{itc2007} rules. \gls{hc} is executed after \gls{sa} until the time limit is reached.\\
\\
The \gls{hc} implementation is under \verb+SimulationAnnealing+ class' \verb+OnlyBetter+ method.\\

\section{Neighborhood Operators}
\label{sec:NeighborhoodOperators}

Neighborhood operators are operations applied to a solution, in order to create other valid solutions (neighbor solutions), but not necessarily feasible. In this context, the core of all operations are the relocation of the examinations.\\
\\
The implementation of neighborhood selection went through three different approaches. Firstly, the random selection was implemented. This approach always chooses a random operator to generate a new neighbor. Thereafter two guided approaches were implemented. The first one raised the probability of selecting one operator if this one generated a better neighbor solution. The probability of that operator is reduced in an equal amount if the operator generated a worse solution. The other guided approach simply lowers the probability of an operator in case of success, and raises it in case of unsuccess. \\
\\
The second guided approach presented worse results compared to the first one, as expected. Oddly, the random approach almost always showed better results compared to the first guided approach, even after suffering major changes in order to try to get it to generate better results. These changes include raising the probabilities even more or less when successfully or unsuccessfully created a better solution, and instead of lowering the probability for an operator when created a worse solution, it is returned to its default value.\\
\\
The neighborhood operators, in this context, are all based on moving examinations to another place (period or room). This implementation uses six different neighborhood operators:
\\
\begin{itemize}
	\item \textit{Room Change} - A random examination is selected. After that, a random room is randomly selected. If the assignment of the random examination to the random room, while maintaining the period, does not violate any hard constraints, that neighbor is returned. If not, the adjacent rooms are tested until one of them creates a feasible solution. If it reaches the limit of rooms and no feasible solution is found, no neighbor is returned;\\
	
	\item \textit{Period Change} - A random examination is selected. After that, a random period is randomly selected. If the assignment of the random examination to the random period, while maintaining the room, does not clash with any hard constraints, that neighbor is returned. If not, the adjacent periods are sequentially tested until one of them creates a feasible solution. If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Change} - A random examination is selected. After that, a random room and period are randomly selected. If the assignment of the random examination to the random room and period does not clash with any hard constraints, that neighbor is returned. If not, the next rooms are tested for each of the next periods, until one of them creates a feasible solution. If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Room Swap} - A random examination is selected. After that, a random room is selected. If the selected examination can be placed in that room, while maintaining the period, then a \textit{Room Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random room, keeping the same period, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next rooms are tested until a feasible solution is found (always testing first if a \textit{Room Change} can be returned instead). If it reaches the limit of rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period Swap} -  A random examination is selected. After that, a random period is selected. If the selected examination can be placed in that period, while maintaining the room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period, keeping the same room, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods are tested until a feasible solution is found (always testing first if a \textit{Period Change} can be returned instead). If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Swap} - A random examination is selected. After that, a random period and room are selected. If the selected examination can be placed in that period and room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period and room does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods and rooms are tested until a feasible solution is found (always testing first if a \textit{Period \& Room Change} can be returned instead). If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;
\end{itemize}

\subsection{Implementation}

The original concept of neighbor solution is to have another solution apart from the current one, which is the result of applying the neighborhood operator to the current solution. In order to have an efficient implementation, the neighbor only keeps the changes introduced in the solution, and not the changed solution. With this design, there's no need to replicate the original solution and apply the neighborhood operator to it in order to obtain the neighbor solution. The process of replacing the original solution with the neighbor, consists in applying to the current solution the movement registered in the neighbor.\\
\\
Every neighbor object must implement the interface \verb+INeighbor+, which presents the methods \verb+Accept+, \verb+Reverse+ and a real value that represents the fitness of the neighbor (the fitness of the new solution if this neighbor is to be accepted). The \verb+Accept+ method  moves the current solution to the neighbor solution. The \verb+Reverse+ method undoes the operation, getting then back the old solution. The different neighbors and their methods are illustrated in Figure \ref{fig:Neighbors}.\\
\begin{figure}[b!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 

\umlclass[x = 0, y = 0]{SimulatedAnnealingTimetable}{}{}
\umlclass[x = 0, y = -4]{NeighborSelectionTimetable}
{
	-examinations : Examinations\\
	-rooms : Rooms\\
	-periods : Periods\\
	-feasibility\_tester : FeasibilityTester\\
	-evaluation\_function\_timetable : EvaluationFunctionTimetable\\
}
{
	+RoomSwap(solution : Solution) : INeighbor\\
	+PeriodSwap(solution : Solution) : INeighbor\\
	+PeriodRoomSwap(solution : Solution) : INeighbor\\
	+RoomChange(solution : Solution) : INeighbor\\
	+PeriodChange(solution : Solution) : INeighbor\\
	+PeriodRoomChange(solution : Solution) : INeighbor\\
}
\umlclass[x = 0, y = -9]{INeighbor}
{
	+fitness : int\\
}
{
	+Accept() : ISolution\\
	+Reverse() : ISolution
}
\umlclass[x = -3, y = -11]{PeriodChangeNeighbor}{}{}
\umlclass[x = -3, y = -13]{RoomChangeNeighbor}{}{}
\umlclass[x = -3, y = -15]{PeriodRoomChangeNeighbor}{}{}
\umlclass[x = 3, y = -11]{PeriodSwapNeighbor}{}{}
\umlclass[x = 3, y = -13]{RoomSwapNeighbor}{}{}
\umlclass[x = 3, y = -15]{PeriodRoomSwapNeighbor}{}{}


\end{umlpackage}



\umlimpl[geometry=-|, anchors=180 and -90]{PeriodChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{RoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{PeriodRoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{RoomSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodRoomSwapNeighbor}{INeighbor}
\umlinclude[anchors=-90 and 90, name=uses]{SimulatedAnnealingTimetable}{NeighborSelectionTimetable}
\umlinclude[anchors=-90 and 90, name=uses]{NeighborSelectionTimetable}{INeighbor} 
\end{tikzpicture}

\caption{Neighborhood selection and operators} 
\label{fig:Neighbors}
\end{figure}