\chapter{Proposed Approach: Local Search}
\label{chap:LocalSearch}
\thispagestyle{plain}

The proposed solution consists about using a local search meta-heuristic(s) to improve the solution given by the \gls{gc} heuristic. This approach uses \gls{sa}, based on M\"{u}ller's approach \cite{Mueller2009}, followed by the use of \gls{hc}. The use of \gls{hc} is justified by the fact that \gls{sa} does not guarantee a good control of the execution time, and so the parameters are given to make it run almost all the available time. The rest of the optimization is executed on \gls{hc}, whose execution time is perfectly controllable.

\section{Simulated Annealing}
\label{sec:SimulatedAnnealing}

\gls{sa} is a single-solution meta-heuristic (section \ref{subsec:MetaHeuristics}). This meta-heuristic optimizes a solution by generating neighbor solutions which might be accepted given an acceptance criterion. A neighbor solution is obtained by applying a movement operator (also known as neighbor operator) to the current solution, creating a new solution which is a neighbor of the current solution. A neighbor operator, in this context, could be the movement of an examination to another time slot. Being a single-solution meta-heuristic, it only generates one neighbor at the time. The neighbor operators and acceptance criterion are the most important part of this algorithm. Little changes on one of these, may get the algorithm to behave in very different ways and end up with much different solutions.\\
\\
The acceptance criterion will, considering the current solution and a neighbor solution, give the percentage of acceptance of the neighbor solution. Most of the approaches using this heuristic accept a new neighbor solution if this is \textit{better} than the current solution. Otherwise, there's a chance that the neighbor solution is still accepted, depending on certain parameters. These parameters are the \textit{Temperature} (normally given as maximum and minimum temperature) and the \textit{Cooling Schedule}. By definition, the higher the temperature, the higher is the chance to accept a worse solution over the current solution. The cooling schedule, as the name suggests, is a function that lowers the temperature at a given rate. The SA algorithm finishes when the current temperature is lower or equal to the minimum temperature. The temperature should start high enough in order to be able to escape from local optima, by accepting worse solutions found during the trajectory.

\subsection{Implementation}

The \gls{sa} was implemented in a way that it is independent from the type of the cooling schedule and neighbor generator. The \gls{sa} base class is abstract and implements everything except for the neighbor generator, which is an abstract method that must be implemented in order to decide how the neighbor generator behaves. It also does not implement the evaluation function (the one that computes the fitness value of a solution or neighbor). The \verb+SimulatedAnnealing+ and \verb+SimulatedAnnealingTimetable+'s methods and variables can be seen in Figure \ref{fig:SimulatedAnnealing}.\\
\\
\begin{figure}[p!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 
\umlabstract[x = 0]{SimulatedAnnealing}
{
	\umlvirt{\#evaluation\_function : IEvaluationFunction}\\
	-cooling\_schedule : ICoolingSchedule
}
{
	+Exec(solution : ISolution, TMax : double, \\ TMin : double, loops : int, type : int, minimize : bool)\\
	+Exec2(solution : ISolution, TMax : double, \\ TMin : double, loops : int, rate : double, type : int, minimize : bool)\\
	+ExecLinearTimer(solution : ISolution, TMax : double,\\ TMin : double, milliseconds : long, type : int, long : minimize)\\
	\umlvirt{\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor}\\
	\umlvirt{\#InitVals(type : int)}
}
\umlclass[x = 0, y = -8]{SimulatedAnnealingTimetable}
{
	\#evaluation\_function : IEvaluationFunction\\
	-neighbor\_selection\_timetable : NeighborSelectionTimetable\\
	+\umlstatic{type\_random : int}\\
	+\umlstatic{type\_guided1 : int}\\
	+\umlstatic{type\_guided2 : int}\\
	-room\_change : int\\
	-period\_change : int\\
	-period\_room\_change : int\\
	-room\_swap : int\\
	-period\_swap : int\\
	-period\_room\_swap : int\\
	+generated\_neighbors : int\\
	-random : Random\\
	-total\_neighbor\_operators : int
}
{
	\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor\\
	\#GenerateNeighbor(solution : Solution, type : int) : INeighbor\\
	-GenerateRandomNeighbor(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor1(solution : Solution) : INeighbor\\
	-GenerateGuidedNeighbor2(solution : Solution) : INeighbor\\
	\#InitVals(type : int)\\
	+EstimateTotalNumberOfNeighbors(average\_reps : int, \\ total\_time : int, solution : Solution)
}
\end{umlpackage}


\umlimpl[anchors=90 and -90]{SimulatedAnnealingTimetable}{SimulatedAnnealing} 
\end{tikzpicture}

\caption{Simulated Annealing classes} 
\label{fig:SimulatedAnnealing}
\end{figure}
The \verb+SimulatedAnnealing+ abstract class has the methods \verb+Exec+, \verb+Exec2+, and \verb+ExecLinearTimer+, which are all similar, but were created to test different approaches. All these methods share the same code, in exception to the cooling schedule (the way the temperature is updated) and acceptance criterion. The pseudo code of these can be seen in Algorithm \ref{alg:SimulatedAnnealing}.\\
\begin{algorithm}[t!]
\textbf{Input:} 
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\item $s$ \Comment{Initial solution}
	\item $TMax$ \Comment{Maximum temperature}
	\item $TMin$ \Comment{Minimum temperature}
	\item $loops$ \Comment{Number of loops per temperature}
\end{itemize}
\begin{algorithmic}
\State $T = Tmax$ ; \Comment{Starting temperature}
\State $Ac = AcInit()$ ; \Comment{Acceptance criterion initializer}
\Repeat
	\Repeat	
		\State Generate a random neighbor $s'$;
		\State $\delta E$ = $f(s') - f(s)$ ;
		\State \textbf{If} $E \leq 0$ \textbf{Then} $s = s'$ \Comment{Accept the neighbor solution}
		\State \textbf{Else} Accept $s'$ with a probability computed using the $Ac$;
	\Until Number of iterations reach $loops$
	\State $T = g(T )$ \Comment{Temperature update}
\Until $T < TMin$
\State \textbf{Output:} $s$ \Comment{return the current (best) solution}
\end{algorithmic}
\caption{Simulated Annealing method.}
\label{alg:SimulatedAnnealing}
\end{algorithm}\\
The \verb+ExecLinearTimer+ has a linear cooling schedule, which is proportional to the running time, and uses the acceptance criterion:\\
\[P(\delta E, T) = e^{\frac{-\delta E}{T}} \]
T $\rightarrow$ Current temperature\\
$\delta$E $\rightarrow$ Fitness difference between the new neighbor and current solution\\
\\
The \verb+Exec+ method shares the same acceptance criterion but uses a geometric cooling schedule:\\
\[T (i+1) = T(i).r \]
r $\rightarrow$ Temperature decreasing rate (0 < r < 1).\\
\\
The rate must belong in the interval ]0,1[. In the geometric cooling schedule, the closer the rate is to unity, the longer the algorithm takes to finish and wider is the area of solutions to be analyzed in the solution space.\\
\\
The \verb+Exec2+ method is the one used in this project. It uses an exponential (decreasing) cooling schedule \cite{CarvalhoLisbonNovember2004}:\\
\[T = T_{max}e^{-R.t} \]
t $\rightarrow$ Current span, counter initiated at 0\\
T\textsubscript{max} $\rightarrow$ Maximum/initial temperature\\
R $\rightarrow$ Decreasing rate.\\
\\
This method also uses a different acceptance criterion:\\
\[P(\delta E, T, SF) = e^{\frac{-\delta E}{T.f(s)}} \]
f(s) $\rightarrow$ Solution fitness\\
\\
\subsection{Variable Rate Computation}

\textcolor{green}{As mentioned previously in Section \ref{sec:SolutionInitResults}, according to the \gls{itc2007} rules, the allowed time considering the used computer is 221000 milliseconds. As this time limit is the only imposition, it was decided to implement a \gls{sa} with an adaptive cooling schedule, in order to use all the available time for the optimization process independently of the chosen dataset. This approach contrasts with the one in which a fixed cooling schedule is used. Each set has its own characteristics and using a given set of parameters for one set does not guarantee that the results will be equivalently good for the other sets. For example, suppose that we've determined the best parameters for the first set, considering a time limit of 200000 milliseconds. Running the algorithm using the same parameters for the remaining sets will not terminate on the time limit of 200000 milliseconds: the sets with less number of resources (exams, rooms, students) will be optimized using less time; on the other way, the largest datasets will demand more time, eventually running over the imposed time limit.\\
\\
Hence, a SA with a variable rate was implemented to make this heuristic run closer to the given time limit for all the datasets. Considering it is not certain that the algorithm will run within the given 221000 milliseconds, because of its stochastic nature, a time limit was added to this meta-heuristic as well, ending this heuristic automatically if the time limit is reached. To avoid the performance degradation incurred by the algorithm's halting before reaching the end of the optimization, a time offset was imposed. This offset is a percentage of the total allowed algorithm execution time. For example, instead of letting the heuristic run for 221000 milliseconds, one allows it to run for 185640 milliseconds, to have a margin of safety. The offset used is 16%.\\
\\
In order to determine the proper cooling schedule, the algorithm starts by simulating the execution of this heuristic, by running all the neighbor operators \textit{n} times. Using the elapsed time and the given time limit, we estimate the number of neighbors that would be generated if this heuristic was to be run within the time limit. To calculate the number of total neighbors, we use the expression: $CompNeighbs = TotalTime / CompTime * AverageReps * TotalOperats$. After that, we compute the exact number of the neighbors ($TotalNeighbs$) that will be generated for the given parameters, using the default rate. This is achieved by simulating this heuristic using those parameters, cooling the current temperature until it reaches the minimum and returning the number of desired generated neighbors. If the $TotalNeighbs$ is way higher than the $CompNeighbs$, a lower rate will be used to make another simulation, until the $TotalNeighbs$ for the given rate reaches a value that is close to the $TotalNeighbs$. The pseudo code of this method can be seen in Algorithm \ref{alg:RateComputing}.\\
\\
\begin{algorithm}[t!]
\textbf{Input:} 
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\item $s$ \Comment{Solution}
	\item $TMax$ \Comment{Maximum temperature}
	\item $TMin$ \Comment{Minimum temperature}
	\item $reps$ \Comment{Number of loops per temperature}
	\item $exec\_time$ \Comment{Execution time limit}
\end{itemize}
\begin{algorithmic}
\State $sa = SAInit()$ ;
\State $n = 50$ ; \Comment{Number of times each operator runs}
\State $est\_neighbors = sa.EstimateNumberNeighbors(n, exec\_time, s)$ ;
\State $rate = 1.50^{-02}$ ;\Comment{Initial default rate}
\State $power = -3$ ;
\State $rate\_to\_sub = 10^{power}$ ;\Comment{Rate decrementing}
\State $total\_neighbors = sa.GetNumberNeighbors(TMax, rate, reps, TMin)$ ;
\Repeat	
	\State \textbf{If} $rate - rate\_to\_sub \leq 0$ \textbf{Then} $power = power - 1$ ; $rate\_to\_sub = 10^{power}$ ;
	\State $rate = rate - rate\_to\_sub$ ;
	\State $total\_neighbors = sa.GetNumberNeighbors(TMax, rate, reps, TMin)$ ;
\Until $total\_neighbors < est\_neighbors$
\State $rate = rate + rate\_to\_sub$ ; \Comment{To guarantee that total\_neighbors < est\_neighbors}
\State \textbf{Output:} $rate$ \Comment{Return computed rate}
\end{algorithmic}
\caption{Rate computing.}
\label{alg:RateComputing}
\end{algorithm}Some testings were made in order to check this heuristic's behavior, using the following parameters: \textit{TMax} = 0.1, \textit{TMin} = $1e-06$, \textit{loops} = 5, \textit{exec\_time} = 50000, and the computed\_rate = 0.00016. As can be seen in Figure \ref{fig:SimulatedAnnealingPlot}, it starts by accepting all worse and better neighbor solutions. In the end, the temperature is so low that it becomes harder to accept worse solutions, ending up acting similar to the \gls{hc} procedure.}\\
\\
\begin{figure}[!t]
\centering

\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={Neighbor index},
    ylabel={Solution score},
    mark size=0.2pt
]

\addplot plot file [
    color=blue,
    mark=*,
    mark options={solid},
   	smooth
    ] {sa_plot_data.dat};

\end{axis}
\end{tikzpicture}

\caption{Simulated Annealing results: score value as a function of the temperature} 
\label{fig:SimulatedAnnealingPlot}
\end{figure}

\section{\textcolor{green}{Hill Climbing}}
\label{sec:HillClimbing}

\gls{hc} is a meta-heuristic different from the \gls{sa} family of meta-heuristics, in the sense that it only accepts improving solutions. So, as long as it reaches a local optimum, it can't get out of that point because all neighbor solutions are worse. In this way, \gls{hc} only has one parameter that is the number of iterations or time limit which controls the algorithm's execution time.\\
\\
In the evaluation undertaken, the best results were obtained with the \verb+Exec2+ version of \gls{sa}. \gls{sa} was parametrized in order to finish execution within the specified time limit imposed by the \gls{itc2007} rules. \gls{hc} is executed after \gls{sa}, using the remaining time, until the time limit is reached.

\subsection{Implementation}
The implementation of this heuristic is very similar to the \gls{sa}, also containing the classes \verb+HillClimbing+ and \verb+HillClimbingTimetable+, which are simplified versions of the \gls{sa} classes. The \verb+HillClimbing+ and \verb+HillClimbingTimetable+'s methods and variables can be seen in Figure \ref{fig:HillClimbing}.\\
\\
\begin{figure}[t!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 
\umlabstract[x = 0]{HillClimbing}
{
	\umlvirt{\#evaluation\_function : IEvaluationFunction}\\
}
{
	+Exec(solution : ISolution, milliseconds : long, type : int, \\ minimize : bool)\\
	\umlvirt{\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor}
}
\umlclass[x = 0, y = -5]{HillClimbingTimetable}
{
	\#evaluation\_function : IEvaluationFunction\\
	-neighbor\_selection\_timetable : NeighborSelectionTimetable\\
	+\umlstatic{type\_random : int}\\
	-random : Random\\
	+generated\_neighbors : int\\
	-total\_neighbor\_operators : int\\
}
{
	\#GenerateNeighbor(solution : ISolution, type : int) : INeighbor\\
	\#GenerateNeighbor(solution : Solution, type : int) : INeighbor\\
	-GenerateRandomNeighbor(solution : Solution) : INeighbor\\
}
\end{umlpackage}


\umlimpl[anchors=90 and -90]{HillClimbingTimetable}{HillClimbing} 
\end{tikzpicture}

\caption{Hill Climbing classes} 
\label{fig:HillClimbing}
\end{figure}

\section{Neighborhood Operators}
\label{sec:NeighborhoodOperators}

Neighborhood operators are operations applied to a solution, in order to create other valid solutions (neighbor solutions), but not necessarily feasible. In this context, the core of all operations are the relocation of the examinations.\\
\\
The implementation of neighborhood selection went through different approaches. Firstly, the random selection was implemented. This approach always chooses a random operator to generate a new neighbor. Thereafter, guided approaches were implemented. The first one raised the probability of selecting one operator if this one generated a better neighbor solution. The probability of that operator is reduced in an equal amount if the operator generated a worse solution.\\
\\
The authors have implemented other variations of the first approach, where increasingly higher/lower probabilities of neighbor selection were defined based on the success/failure of the operator. Despite this, the random approach almost always showed better results compared to the guided approaches.\\
\\
The neighborhood operators, in this context, are all based on moving examinations to another place (period or room). This implementation uses six different neighborhood operators:
\\
\begin{itemize}
	\item \textit{Room Change} - A random examination is selected. After that, a random room is randomly selected. If the assignment of the random examination to the random room, while maintaining the period, does not violate any hard constraints, that neighbor is returned. If not, the adjacent rooms are tested until one of them creates a feasible solution. If it reaches the limit of rooms and no feasible solution is found, no neighbor is returned;\\
	
	\item \textit{Period Change} - A random examination is selected. After that, a random period is randomly selected. If the assignment of the random examination to the random period, while maintaining the room, does not clash with any hard constraints, that neighbor is returned. If not, the adjacent periods are sequentially tested until one of them creates a feasible solution. If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Change} - A random examination is selected. After that, a random room and period are randomly selected. If the assignment of the random examination to the random room and period does not clash with any hard constraints, that neighbor is returned. If not, the next rooms are tested for each of the next periods, until one of them creates a feasible solution. If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Room Swap} - A random examination is selected. After that, a random room is selected. If the selected examination can be placed in that room, while maintaining the period, then a \textit{Room Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random room, keeping the same period, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next rooms are tested until a feasible solution is found (always testing first if a \textit{Room Change} can be returned instead). If it reaches the limit of rooms and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period Swap} -  A random examination is selected. After that, a random period is selected. If the selected examination can be placed in that period, while maintaining the room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period, keeping the same room, does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods are tested until a feasible solution is found (always testing first if a \textit{Period Change} can be returned instead). If it reaches the limit of periods and no feasible solution was found, no neighbor is returned;\\
	
	\item \textit{Period \& Room Swap} - A random examination is selected. After that, a random period and room are selected. If the selected examination can be placed in that period and room, then a \textit{Period Change} neighbor is returned instead. If not, if the swapping of the random examination with any of the examinations presented in the random period and room does not clash with any hard constraints, that neighbor is returned. If not, the examinations presented in the next periods and rooms are tested until a feasible solution is found (always testing first if a \textit{Period \& Room Change} can be returned instead). If it reaches the limit of periods and rooms and no feasible solution was found, no neighbor is returned;
\end{itemize}

\subsection{Implementation}

The original concept of neighbor solution is to have another solution apart from the current one, which is the result of applying the neighborhood operator to the current solution. In order to have an efficient implementation, the neighbor only keeps the changes introduced in the solution, and not the changed solution. With this design, there's no need to replicate the original solution and apply the neighborhood operator to it in order to obtain the neighbor solution. The process of replacing the original solution with the neighbor, consists in applying to the current solution the movement registered in the neighbor.\\
\\
Every neighbor object must implement the interface \verb+INeighbor+, which presents the methods \verb+Accept+, \verb+Reverse+ and a real value that represents the fitness of the neighbor (the fitness of the new solution if this neighbor is to be accepted). The \verb+Accept+ method  moves the current solution to the neighbor solution. The \verb+Reverse+ method undoes the operation, getting then back the old solution. The different neighbors and their methods are illustrated in Figure \ref{fig:Neighbors}.\\
\begin{figure}[p!]
\centering
\begin{tikzpicture}
\begin{umlpackage}[x = 2, y = 0]{Heuristics Layer} 

\umlclass[x = 0, y = 0]{SimulatedAnnealingTimetable}{}{}
\umlclass[x = 0, y = -4]{NeighborSelectionTimetable}
{
	-examinations : Examinations\\
	-rooms : Rooms\\
	-periods : Periods\\
	-feasibility\_tester : FeasibilityTester\\
	-evaluation\_function\_timetable : EvaluationFunctionTimetable\\
}
{
	+RoomSwap(solution : Solution) : INeighbor\\
	+PeriodSwap(solution : Solution) : INeighbor\\
	+PeriodRoomSwap(solution : Solution) : INeighbor\\
	+RoomChange(solution : Solution) : INeighbor\\
	+PeriodChange(solution : Solution) : INeighbor\\
	+PeriodRoomChange(solution : Solution) : INeighbor\\
}
\umlclass[x = 0, y = -9]{INeighbor}
{
	+fitness : int\\
}
{
	+Accept() : ISolution\\
	+Reverse() : ISolution
}
\umlclass[x = -3, y = -11]{PeriodChangeNeighbor}{}{}
\umlclass[x = -3, y = -13]{RoomChangeNeighbor}{}{}
\umlclass[x = -3, y = -15]{PeriodRoomChangeNeighbor}{}{}
\umlclass[x = 3, y = -11]{PeriodSwapNeighbor}{}{}
\umlclass[x = 3, y = -13]{RoomSwapNeighbor}{}{}
\umlclass[x = 3, y = -15]{PeriodRoomSwapNeighbor}{}{}


\end{umlpackage}



\umlimpl[geometry=-|, anchors=180 and -90]{PeriodChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{RoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=180 and -90]{PeriodRoomChangeNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{RoomSwapNeighbor}{INeighbor} 
\umlimpl[geometry=-|, anchors=-180 and -90]{PeriodRoomSwapNeighbor}{INeighbor}
\umlinclude[anchors=-90 and 90, name=uses]{SimulatedAnnealingTimetable}{NeighborSelectionTimetable}
\umlinclude[anchors=-90 and 90, name=uses]{NeighborSelectionTimetable}{INeighbor} 
\end{tikzpicture}

\caption{Neighborhood selection and operators} 
\label{fig:Neighbors}
\end{figure}

\subsection{\textcolor{green}{Statistics}}
\label{sub:SAStatistics}
It is important to make sure that the algorithm works as planned. This includes being stochastic, and all the neighbor operators must contribute positively to obtain better results.\\
\\
In this particular case, to make sure this heuristic is stochastic, we must guarantee that all the examinations are moved more or less the same amount of times. Considering some examinations are harder to move, these are moved less times than others, but the difference is not significant. A study was made to compare, in the same run, the number of rejected and accepted neighbor for each examination. The examinations are sorted in descending order by the number of conflicts. Figure \ref{fig:AvsRNeighbors} represents a color map in which the colors represent the number of times each examination was accepted (x = 1) and rejected (x = 2), for set 1. As can be seen, some examinations are not even selected to move, and so, have zero accepted and rejected results. In this particular set, only two lines are completely blue, as checked in the results file. These two examinations have \textit{exam\_coincidence} hard constraint and are only able to be in one room because of the number of students being so high. The \gls{sa} algorithm is not optimized to move more than one examination at the same time. This means that those two examinations can never be moved to another period, because the \gls{sa} can only move one examination at a time and these examinations can't be moved to another room because they don't fit in other rooms.\\
\\
Other blue lines, which have values between 1000 and 1, are rare. This occurs not only because the examination is hard to move due to its conflicts, but may also have the \textit{exam\_coincidence} and a really short number of rooms which that examination can fit. This problem creates a limitation in the \gls{sa} heuristic, preventing the \gls{sa} from scanning parts of the solutions space, and eventually degrading the quality of the final result.\\
\\
\begin{figure}[t!]
\centering

\caption{Accepted and rejected neighbors for each examination in the Simulated Annealing.} 
\label{fig:AvsRNeighbors}

\begin{tikzpicture}

	\begin{axis}[
		colorbar style={
		scaled y ticks = false,
		y tick label style={/pgf/number format/fixed,
			/pgf/number format/1000 sep = \thinspace 	% Optional if you want to
														% replace comma as the 1000
														% separator 
        }
		},
		width = \textwidth,
		xlabel = {Accepted (x=1) and rejected (x=2) neighbors},
		ylabel = {Exam indexes},
		enlarge x limits = true,	% The value true enlarges the lower 
									% and upper limit.
									% The value false uses tight axis limits
		enlarge y limits = true,
		colorbar,
		%colormap={}{ gray(0cm)=(1); gray(1cm)=(0);}, % Grey colorbar
		%x dir = reverse,
		y dir = reverse,
		%xmode = log,        % logarithmic x axis
		%xmin = 0, xmax = 9,
		% car92 has 543 exams
		%ymin = 0, ymax = 606, 
%		ytick = { 0, 100, 200, 300, 400, 500, 542},
		extra x ticks = {
1,
2,
3
}, extra x tick labels = {},
	    		extra tick style = { grid = major},
	    		width = \textwidth,
	    		%width = 17cm,
			%height = 11cm,
		]
		\addplot[surf, 
			shader = interp, % No facets
			point meta = explicit] 
			%table [z expr = 100, meta index = 2] {./yor83Cool1Eminus6.dat};
			table [z expr = 100, meta index = 2] {./SAResults6.dat};
\end{axis}
\end{tikzpicture}
\end{figure}
\subsection{Neighborhood Operators Effect}
One of the most important aspects when dealing with the \gls{sa} is the choice of the neighbor operators. It is crucial that all the neighbor operators contribute positively. If a neighbor operator contributes negatively, it's better not to use it. M\"{u}ller proposed the first five operators~\cite{Mueller2009} \textit{Room Change}, \textit{Period Chage}, \textit{Period \& Room Change}, \textit{Room Swap}, and \textit{Period Swap}. We add to the Muller's set a sixth operator, the \textit{Period \& Room Swap} operator. In the sequel we present a study of the influence of this operator. For all 12 sets, 20 runs were performed, calculating the fitness average for each of the two cases. Table \ref{tab:NeighborsImprovementFactor} shows the improvement factor for using six neighbors instead of five. In most cases it's worse compared to using the original five neighbor operators, and so, the 6th operator will not be used in this project's results. The results of using five operators also showed that more neighbors were generated and the percentage of feasible generated neighbors compared to the unfeasible, for each set, is also higher compared to using six neighbor operators.

\begin{table}[!t]
\centering
\caption{Percentage of improvement factor between using five and six neighbors.}
\sisetup{table-alignment=center,detect-weight=true,detect-inline-weight=math}
\begin{tabular}{%
	S[table-figures-integer=3]%
	S[table-format=5.1]%
	S[table-format=5.1]%
	S[table-format=5.1]%
    }

\toprule

\multicolumn{1}{c}{Set} & \multicolumn{1}{c}{5} &	\multicolumn{1}{c}{6} &	\multicolumn{1}{c}{Improvement}\\
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Neighbors} &	\multicolumn{1}{c}{Neighbors}&	\multicolumn{1}{c}{Factor (\%)}\\

\midrule

1   &   5019  & 5055 & \textcolor{red}{0.7$\downarrow$}\\
2   &   478  & 523 & \textcolor{red}{9.4$\downarrow$} \\
3   &   14997  & 15228 & \textcolor{red}{1.5$\downarrow$} \\
4   &   \text{--}  & \text{--} & \text{--}\\
5   &   4950 & 4819 & \textcolor{green}{1.5$\uparrow$}\\
6   &   27680  & 27528 & \textcolor{green}{0.5$\uparrow$} \\
7   &   4799  & 4812 & \textcolor{red}{0.3$\downarrow$} \\
8   &   8551  & 8741 & \textcolor{red}{2.2$\downarrow$} \\
9   &   1141  & 1150 & \textcolor{red}{0.8$\downarrow$} \\
10  &   28219  & 25586 & \textcolor{green}{9.3$\uparrow$} \\
11  &   45344  & 45630 & \textcolor{red}{0.6$\downarrow$} \\
12  &   7222  & 7679 & \textcolor{red}{6.3$\downarrow$} \\

\bottomrule

\end{tabular}
\label{tab:NeighborsImprovementFactor}
\end{table}


\section{\textcolor{green}{Fitness Computation}}
\label{sec:FitnessComputation}

Fitness evaluation in timetable automation, due to its complexity, has a great influence in the algorithm performance. Hence, good performance of this step is required. As mentioned in the Subsection \ref{subsec:ToolsLayer}, this value is computed using the \verb+EvaluationFunction+ tool, which in a first approach, implied for generated neighbors, the recalculation of their fitness value. Tests were made to study this approach since the fitness results were not as good as expected. The tests showed that this approach was having poor performance since most of the time was used to compute the fitness for each new neighbor generated solution.\\
\\
A new approach was implemented, which consists in computing the fitness incrementally. This means that for each new generated neighbor, a new fitness value is computed based on the current solution's fitness. This approach takes advantage of the fact that the operations applied to the current solution are simple and so the neighbor solutions are, in most part, equal to the current solution. Thus, the fitness computation is performed considering only the small changes between the current solution and the new generated neighbor solution.\\
\\
Tests were made to compare both approaches and, using the same parameters, for the first set it was possible to achieve equivalent results, nineteen times faster. This means that if both approaches would use the same parameters, but the new approach using a lower rate to match the execution time of both approaches, the new approach would have generated nineteen times more neighbors.

\subsection{Implementation}

To compute the fitness of the neighbor solutions incrementally, a fitness function must be implemented depending on the type of neighbor. Considering that all neighbors have a reference to the solution they derive from, and the methods \verb+Accept+ and \verb+Reject+, it's possible to access the current solution and the neighbor generated solution (Reminding that the \verb+Accept+ will change the current solution and so the reference presented in this neighbor, replacing with the neighbor solution). \\
\\
The differences between the fitness values of the current solution and the new neighbor's depend on the type of the neighbor created. For example, if a \verb+RoomSwapNeighbor+ is created, the changes will occur in the mixed durations constraints value of the periods and rooms of the swapped examinations. It is necessary to compute the impact (fitness values) of these conflicts in the current solution and subtract its value to the current fitness, proceeding with computing the same impact but now for the generated neighbor and sum its value to the current fitness, thus obtaining the new neighbor's fitness value. The impact computing mentioned above is a simple rule that is must be followed by all the soft constraints when computing the new neighbor's fitness incrementally.